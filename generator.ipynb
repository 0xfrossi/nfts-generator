{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "os.makedirs(\"./assets\", exist_ok=True)\n",
    "os.makedirs(\"./layers_data_json\", exist_ok=True)\n",
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "\n",
    "path_project_dir=\"./assets/\"\n",
    "\n",
    "#Validation of 'schemas.json', check if some values make sense\n",
    "def validate_schemas_file():\n",
    "    with open(\"schemas.json\") as user_file:\n",
    "        config = json.load(user_file)\n",
    "\n",
    "    check=True\n",
    "    \n",
    "    for k, weights_list in config[\"weight_traits\"].items(): \n",
    "        weights=np.array(list(weights_list) ,dtype=np.float32)\n",
    "\n",
    "        round_sum=\"{:.2f}\".format(sum(weights))\n",
    "        result= np.float32(round_sum)==np.float32(1.00)\n",
    "\n",
    "        try:\n",
    "            location=os.path.join(\"./assets/\", k)\n",
    "            files_list = os.listdir(location)\n",
    " \n",
    "        except FileNotFoundError:\n",
    "        \n",
    "            print(f\"Directory {location} doesn't exist.\")\n",
    "            return 0\n",
    "        \n",
    "\n",
    "        if not result:\n",
    "\n",
    "            print(f'Total sum of weights for {k} is not 1!')\n",
    "            check=False\n",
    "        if not (len(files_list) == len(weights)):\n",
    "            print(f'Directory or weights array of {k} with different sizes!')\n",
    "            check=False\n",
    "\n",
    "    for folder_corrente, nomi_sottocartelle, list_nomi_file in os.walk(\"assets\"):\n",
    "        if not nomi_sottocartelle:\n",
    "            for f in list_nomi_file:\n",
    "                img= Image.open(os.path.join(folder_corrente,f))\n",
    "                w,h=img.size\n",
    "                if not (w==config[\"width_images\"] and h==config[\"height_images\"]):\n",
    "                    print(f'image {f} has sizes of {w}x{h}, sizes declared in config are {config[\"width_images\"]}x{config[\"height_images\"]}')\n",
    "                    check=False\n",
    "    if check:\n",
    "        print(\"Images sizes checks: OK\")\n",
    "        print(\"Weights checks: OK \")\n",
    "        print(\"No errors in the config schema\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create a file contaning a path for each image's trait       \n",
    "def create_jsons(path=\"assets\"):\n",
    "    data_json={}\n",
    "    os.makedirs(\"layers_data_json\",exist_ok=True)\n",
    "    for folder_corrente, nomi_sottocartelle, list_nomi_file in os.walk(path):\n",
    "        if not nomi_sottocartelle:\n",
    "            for f in list_nomi_file:\n",
    "        \n",
    "                data_json[os.path.splitext(f)[0]]= os.path.normpath(os.path.join(folder_corrente, f))\n",
    "        \n",
    "            with open(os.path.join(\"layers_data_json\",os.path.basename(folder_corrente) +\".json\"), \"w\") as json_file:\n",
    "                json.dump(data_json, json_file, indent=2)\n",
    "        \n",
    "            data_json.clear()\n",
    "    print(\"Files created in \"+ \"'/layers_data_json' \" )\n",
    "\n",
    "#create a random image combining random different traits for each layer based on their weights               \n",
    "def new_image():\n",
    "    \n",
    "    with open(\"schemas.json\") as user_file:\n",
    "        data_layers = json.load(user_file)\n",
    "    \n",
    "    new_image={}\n",
    "    \n",
    "    select= np.random.choice(len(data_layers[\"weights_schema_in_collection\"]), p= data_layers[\"weights_schema_in_collection\"] )\n",
    "    json_files=data_layers[\"layers_order\"][select]\n",
    "\n",
    "    for f in json_files:\n",
    "        path= os.path.join(\"layers_data_json\", f+\".json\")\n",
    "        with open(path, 'r') as file_json:\n",
    "            json_layer = json.load(file_json)\n",
    "\n",
    "        i=np.random.choice(list(json_layer.keys()),p=data_layers[\"weight_traits\"][os.path.splitext(f)[0]])\n",
    "        new_image[os.path.splitext(os.path.basename(path))[0]]= json_layer[i]\n",
    "        #new_image_data[os.path.splitext(os.path.basename(path))[0]]= i \n",
    "\n",
    "    return new_image\n",
    "   \n",
    "\n",
    "def generate_collection(num_to_generate:int ,name: str, symbol:str,description:str):\n",
    "    \n",
    "    with open(\"schemas.json\") as file:\n",
    "        config=json.load(file)\n",
    "    width= config[\"width_images\"]\n",
    "    height= config[\"height_images\"]\n",
    "\n",
    "    os.makedirs(\"output\",exist_ok=True)\n",
    "    os.makedirs(os.path.join(\"output\",\"img\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(\"output\",\"metadata\"),exist_ok=True)\n",
    "\n",
    "    generated=[]\n",
    "    while len(generated)<num_to_generate:\n",
    "       image= new_image()\n",
    "       if image not in generated:\n",
    "           generated.append(image)\n",
    "    \n",
    "    \n",
    "    data_img={}\n",
    "    edition=0\n",
    "    for image in generated:\n",
    "        traits=[]\n",
    "        base_image= Image.new(\"RGBA\", (width,height), (255, 255, 255, 0))\n",
    "        for k_layer, v_address in image.items():\n",
    "\n",
    "            temp=Image.open(v_address).convert(\"RGBA\") \n",
    "            base_image.paste(temp,(0,0),temp)\n",
    "\n",
    "            second_lvl = os.path.basename(os.path.dirname(v_address))\n",
    "            file_name = os.path.splitext(os.path.basename(v_address))[0]\n",
    "            trait_obj={\"trait_type\": second_lvl , \"value\": file_name}\n",
    "            traits.append(trait_obj)\n",
    "\n",
    "        # metadata json\n",
    "        data_img[\"name\"]= f'{name} #{edition}'\n",
    "        data_img[\"symbol\"]= symbol\n",
    "        data_img[\"description\"]= description\n",
    "        data_img[\"edition\"]= edition\n",
    "        data_img[\"image\"]=f'{edition}.png'\n",
    "        data_img[\"attributes\"]=traits\n",
    "\n",
    "\n",
    "        with open(os.path.join(\"output/metadata\",f'{edition}.json'), \"w\") as json_file:\n",
    "                json.dump(data_img, json_file, indent=2)\n",
    "        \n",
    "        \n",
    "        base_image.save(os.path.join(\"output/img\",f'{edition}.png'))\n",
    "        edition+=1\n",
    "\n",
    "    print(\"Process completed!\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def update_metadatas():\n",
    "    \n",
    "    metadatas=os.listdir(\"output/metadata\")\n",
    "    traits=[os.path.splitext(t)[0] for t in os.listdir(\"layers_data_json\")]\n",
    "\n",
    "    for m in metadatas:\n",
    "        with open(os.path.join(\"output/metadata\",m), \"r+\") as metaFile:\n",
    "            oldFile= json.load(metaFile)\n",
    "            \n",
    "            values=[]\n",
    "            to_add=[]\n",
    "\n",
    "            for attribute in oldFile[\"attributes\"]:\n",
    "                \n",
    "                values.append(attribute[\"trait_type\"])\n",
    "            \n",
    "            for t in traits:\n",
    "                if t not in values:\n",
    "                    to_add.append(t)\n",
    "\n",
    "\n",
    "            for t in to_add:\n",
    "\n",
    "                temp={\"trait_type\": t, \"value\": \"none\"}\n",
    "\n",
    "                oldFile[\"attributes\"].append(temp)\n",
    "            \n",
    "            metaFile.seek(0)\n",
    "\n",
    "            json.dump(oldFile,metaFile, indent=2)\n",
    "            \n",
    "    print(\"Metadatas updated!\")\n",
    "       \n",
    "\n",
    "def create_counts()-> dict:\n",
    "\n",
    "    attributes_count={}\n",
    "    metadatas=os.listdir(\"output/metadata\")\n",
    "    \n",
    "    for m in metadatas:\n",
    "        with open(os.path.join(\"output/metadata\",m), \"r\") as file:\n",
    "\n",
    "            data=json.load(file)\n",
    "            \n",
    "            for attributes in data[\"attributes\"]:\n",
    "                attr_key= (attributes[\"trait_type\"], attributes[\"value\"])\n",
    "                if attr_key not in attributes_count:\n",
    "                   attributes_count[attr_key]=1\n",
    "                else:\n",
    "                    attributes_count[attr_key]+=1\n",
    "\n",
    "    return attributes_count\n",
    "\n",
    "def calculate_percentages(attributes_count:dict)-> dict:\n",
    "\n",
    "    amount=len(os.listdir(\"output/metadata\"))\n",
    "    attributs_percentages={}\n",
    "    for attribute in attributes_count:\n",
    "\n",
    "        freq_percent= round((attributes_count[attribute]/amount)*100, 3)\n",
    "        attributs_percentages[attribute]= freq_percent\n",
    "    \n",
    "    return attributs_percentages\n",
    "\n",
    "\n",
    "def create_rich_metadata(attribute_count: dict, attribute_freq: dict) -> None:\n",
    "    os.makedirs(\"output/rich_metadata\", exist_ok=True )\n",
    "    metadatas=os.listdir(\"output/metadata\")\n",
    "    \n",
    "    for m in metadatas:\n",
    "        with open(os.path.join(\"output/metadata\",m), \"r\") as metaFile:\n",
    "            data= json.load(metaFile)\n",
    "            token_attributes = data['attributes']\n",
    "\n",
    "            for attr in token_attributes:\n",
    "                attr_key = (attr['trait_type'], attr['value'])\n",
    "\n",
    "                count = attribute_count[attr_key]\n",
    "                freq = attribute_freq[attr_key]\n",
    "\n",
    "                attr['count'] = count\n",
    "                attr['frequency'] = freq\n",
    "\n",
    "        data['attributes'] = token_attributes\n",
    "\n",
    "        with open(os.path.join(\"output/rich_metadata/\", str(data[\"edition\"])+\".json\"), 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(data, outfile, indent=2)   \n",
    "    \n",
    "    print(\"Rich metadatas created\")\n",
    "\n",
    "\n",
    "def calculate_mean()->dict:\n",
    "    harmonic_means={}\n",
    "    metadatas=os.listdir(\"output/rich_metadata\")\n",
    "    \n",
    "    for m in metadatas:\n",
    "        with open(os.path.join(\"output/rich_metadata\",m), \"r\") as metaFile:\n",
    "            data= json.load(metaFile)\n",
    "\n",
    "            total_traits = len(data[\"attributes\"])\n",
    "            rarity_sum = 0\n",
    "\n",
    "            for attr in data[\"attributes\"]:\n",
    "                rarity_sum += (1 / attr['count'])\n",
    "            \n",
    "            mean = total_traits / rarity_sum\n",
    "            rounded_mean = round(mean, 3)\n",
    "\n",
    "            harmonic_means[data[\"edition\"]] = rounded_mean\n",
    "\n",
    "        data['rarity'] = dict()\n",
    "        data['rarity']['harmonic'] = dict()\n",
    "        data['rarity']['harmonic']['score'] = rounded_mean\n",
    "\n",
    "        # Opens the original json file and writes the new data\n",
    "        with open(os.path.join(\"output/rich_metadata\", m), 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(data, outfile, indent=2)\n",
    "\n",
    "    return harmonic_means\n",
    "\n",
    "\n",
    "def add_rarity_rank(harmonic_means: dict) -> None:\n",
    "    sorted_tokens = sorted(harmonic_means.items(), key=lambda x: x[1])\n",
    "\n",
    "    rank = 1\n",
    "    for token in sorted_tokens:\n",
    "        #print(token[0])\n",
    "\n",
    "        with open(os.path.join(\"output/rich_metadata\",str(token[0])+\".json\"), 'r+', encoding='utf-8') as infile:\n",
    "\n",
    "            data = json.load(infile)\n",
    "            data['rarity']['harmonic']['rank'] = rank\n",
    "\n",
    "            infile.seek(0)\n",
    "            json.dump(data, infile, indent=2)\n",
    "\n",
    "        rank += 1\n",
    "    \n",
    "    print(\"Rarity ranks added, process completed!\")\n",
    "\n",
    "\n",
    "def summary_rank()->None:\n",
    "\n",
    "    all_dicts=[]\n",
    "    metadatas=os.listdir(\"output/rich_metadata\")\n",
    "    \n",
    "    for  m in metadatas:\n",
    "        with open(os.path.join(\"output/rich_metadata\",m), \"r\") as metaFile:\n",
    "            data= json.load(metaFile)\n",
    "            all_dicts.append(data)\n",
    "\n",
    "    sorted_by_rank = sorted(all_dicts, key=lambda x: x[\"rarity\"][\"harmonic\"][\"rank\"])\n",
    "    rank_edition_dict= {i + 1: data[\"edition\"] for i, data in enumerate(sorted_by_rank)}\n",
    "    with open(os.path.join(\"output/\",\"ranks_score.json\"), 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(rank_edition_dict, outfile, indent=2)\n",
    "\n",
    "    print(\"dictionary 'rank': 'id' created\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files created in '/layers_data_json' \n"
     ]
    }
   ],
   "source": [
    "create_jsons()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images sizes check: OK\n",
      "Weight checks: OK \n",
      "No errors in the config schema\n"
     ]
    }
   ],
   "source": [
    "validate_schemas_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed!\n"
     ]
    }
   ],
   "source": [
    "#Fill the parameters of the function\n",
    "generate_collection(num_to_generate=5555,name=\"Crypto Punks\", symbol=\"PUNKS\",description=\"The punks are coming to get you!\")\n",
    "update_metadatas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional!\n",
    "#Crerate a rich metadata file, calculating means, frequencies, counts and rarity's ranks based on the collection's traits just created\n",
    "attribute_count=create_counts()\n",
    "att_freq=calculate_percentages(attribute_count)\n",
    "create_rich_metadata(attribute_count,att_freq)\n",
    "mean=calculate_mean()\n",
    "add_rarity_rank(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary 'rank': 'id' created\n"
     ]
    }
   ],
   "source": [
    "summary_rank()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
